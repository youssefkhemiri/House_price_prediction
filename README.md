# üè† House Price Prediction System

A comprehensive machine learning system for predicting real estate prices in Tunisia, featuring automated web scraping, data enrichment, model training, and a user-friendly web interface.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.103+-green.svg)
![Machine Learning](https://img.shields.io/badge/ML-Scikit--Learn%20%7C%20XGBoost-orange.svg)
![Web Scraping](https://img.shields.io/badge/Scraping-BeautifulSoup%20%7C%20Requests-red.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)

## üìã Project Description

This project is an end-to-end machine learning solution for predicting real estate prices in the Tunisian market. The system automatically collects property listings from multiple sources, enriches the data using OpenAI's GPT models, trains multiple machine learning models, and provides predictions through both a web interface and API endpoints.

### Key Capabilities

- **ü§ñ Automated Data Collection**: Web scrapers for major Tunisian real estate websites (Menzili, Mubawab)
- **üß† AI-Powered Data Enrichment**: Uses OpenAI GPT to extract structured features from unstructured property descriptions
- **üìä Advanced ML Models**: Multiple algorithms including Random Forest, XGBoost, Linear Regression, and Ensemble methods
- **üåê Web Interface**: User-friendly web application for property price predictions
- **üìà Model Management**: Complete model training, testing, and deployment pipeline
- **‚è∞ Automated Scheduling**: Daily data collection using Celery task scheduler
- **üß™ Comprehensive Testing**: Full test suite with 95%+ coverage

## üèóÔ∏è Project Structure

```
House_price_prediction/
‚îú‚îÄ‚îÄ üìÅ src/                          # Main application source code
‚îÇ   ‚îú‚îÄ‚îÄ app.py                       # FastAPI web application
‚îÇ   ‚îú‚îÄ‚îÄ predict.py                   # Prediction logic
‚îÇ   ‚îú‚îÄ‚îÄ templates.py                 # Template utilities
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                    # Web scraping modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ menzili_scraper.py      # Menzili website scraper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mubawab_scraper.py      # Mubawab website scraper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tecnocasa_scraper.py    # tcnocasa website scraper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ any_website_scraper.py   # Generic website scraper
‚îÇ   ‚îú‚îÄ‚îÄ templates/                   # HTML templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Main page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results.html            # Results display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manual_entry.html       # Manual data entry
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error.html              # Error page
‚îÇ   ‚îî‚îÄ‚îÄ static/                      # CSS and static files
‚îÇ       ‚îî‚îÄ‚îÄ style.css               # Application styling
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                      # Utility and processing scripts
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Model training script
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py              # Model testing and evaluation
‚îÇ   ‚îú‚îÄ‚îÄ demo_models.py              # Interactive model demo
‚îÇ   ‚îú‚îÄ‚îÄ enrich_data.py              # OpenAI data enrichment
‚îÇ   ‚îî‚îÄ‚îÄ crawlers/                   # Automated crawler system
‚îÇ       ‚îú‚îÄ‚îÄ celery_timer.py         # Celery task scheduler
‚îÇ       ‚îú‚îÄ‚îÄ menzili_crawler.py      # Menzili daily crawler
‚îÇ       ‚îú‚îÄ‚îÄ mubaweb_crawler.py      # Mubawab daily crawler
‚îÇ       ‚îú‚îÄ‚îÄ tecnocasa_crawler.py    # tecnocasa crawler
‚îÇ       ‚îú‚îÄ‚îÄ setup_celery.py         # Celery setup automation
‚îÇ       ‚îî‚îÄ‚îÄ test_celery_system.py   # Celery system tests
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                        # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Raw scraped data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ menzili_listings.json  # Menzili property data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mubawab_listings.json  # Mubawab property data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backups/               # Weekly data backups
‚îÇ   ‚îú‚îÄ‚îÄ processed/                 # Processed and enriched data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_real_estate_datav4.json
‚îÇ   ‚îî‚îÄ‚îÄ external/                  # External data sources
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                      # Trained machine learning models
‚îÇ   ‚îú‚îÄ‚îÄ random_forest_*.joblib     # Random Forest model
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_*.joblib           # XGBoost model
‚îÇ   ‚îú‚îÄ‚îÄ linear_regression_*.joblib # Linear Regression model
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_*.joblib          # Ensemble model
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_vectorizer_*.joblib  # Text vectorizer
‚îÇ   ‚îî‚îÄ‚îÄ latest_model_metadata.json # Model metadata
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/                       # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py                # API endpoint tests
‚îÇ   ‚îú‚îÄ‚îÄ test_scrapers.py           # Scraper functionality tests
‚îÇ   ‚îú‚îÄ‚îÄ test_prediction.py         # Prediction logic tests
‚îÇ   ‚îú‚îÄ‚îÄ test_integration.py        # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py                # Test configuration
‚îÇ
‚îú‚îÄ‚îÄ üìÅ logs/                        # Application logs
‚îÇ   ‚îú‚îÄ‚îÄ daily_summary_*.json       # Daily crawler reports
‚îÇ   ‚îú‚îÄ‚îÄ menzili_tasks.jsonl        # Menzili task logs
‚îÇ   ‚îú‚îÄ‚îÄ mubawab_tasks.jsonl        # Mubawab task logs
‚îÇ   ‚îî‚îÄ‚îÄ system_test.jsonl          # System test logs
‚îÇ
‚îú‚îÄ‚îÄ üìã Configuration Files
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ requirements-celery.txt     # Celery-specific dependencies
‚îÇ   ‚îú‚îÄ‚îÄ requirements-test.txt       # Testing dependencies
‚îÇ   ‚îú‚îÄ‚îÄ pytest.ini                 # Test configuration
‚îÇ   ‚îî‚îÄ‚îÄ .gitignore                 # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ üöÄ Deployment & Operations
‚îÇ   ‚îú‚îÄ‚îÄ start_crawler.bat          # Windows crawler launcher
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py               # Test runner script
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Utility functions
‚îÇ
‚îî‚îÄ‚îÄ üìö Documentation
    ‚îú‚îÄ‚îÄ README.md                   # This file
    ‚îú‚îÄ‚îÄ CELERY_CRAWLER_GUIDE.md     # Celery system guide
    ‚îú‚îÄ‚îÄ CELERY_SETUP_COMPLETE.md    # Celery setup instructions
    ‚îú‚îÄ‚îÄ MODEL_SYSTEM_COMPLETE.md    # Model system documentation
    ‚îî‚îÄ‚îÄ README_WebApp.md            # Web application guide
```

## ‚ú® Features

### üîç Web Scraping & Data Collection
- **Multi-source scraping**: Automated collection from Menzili and Mubawab
- **Generic scraper**: Adaptable scraper for any real estate website
- **Intelligent parsing**: Extracts property details, prices, and descriptions
- **Duplicate detection**: Prevents duplicate listings across sources
- **Rate limiting**: Respectful scraping with configurable delays

### üß† AI-Powered Data Enrichment
- **OpenAI Integration**: Uses GPT-4 to extract structured data from descriptions
- **Feature extraction**: Automatically identifies amenities, room counts, and quality scores
- **Location parsing**: Extracts governorate, delegation, and locality information
- **Quality assessment**: AI-generated property quality scores (0-10 scale)
- **Batch processing**: Efficient processing of thousands of listings

### ü§ñ Machine Learning Pipeline
- **Multiple algorithms**: Random Forest, XGBoost, Linear Regression, Ensemble
- **Feature engineering**: Automated creation of derived features
- **Text analysis**: TF-IDF vectorization of property descriptions
- **Model persistence**: Automatic saving and versioning of trained models
- **Performance tracking**: Comprehensive metrics and model comparison

### üåê Web Application
- **FastAPI backend**: High-performance async API
- **Interactive UI**: Bootstrap-based responsive interface
- **Multiple input methods**: URL scraping and manual property entry
- **Real-time predictions**: Instant price predictions with confidence intervals
- **Results visualization**: Clear presentation of predictions and property details

### ‚è∞ Automated Operations
- **Daily scheduling**: Automatic daily data collection at 2:00 AM
- **Task monitoring**: Celery-based task queue with retry logic
- **Error handling**: Robust error handling and notification system
- **Data backups**: Weekly automated backups of all data
- **Log management**: Comprehensive logging and monitoring

### üß™ Testing & Quality Assurance
- **95%+ test coverage**: Comprehensive test suite
- **API testing**: Complete endpoint testing with mock data
- **Integration tests**: End-to-end workflow validation
- **Performance tests**: Load testing and performance monitoring
- **Continuous validation**: Automated model accuracy checks

## üöÄ Setup

### Prerequisites
- **Python 3.8+** (Recommended: Python 3.10+)
- **Redis server** (for Celery task queue)
- **OpenAI API key** (for data enrichment)
- **Git** (for version control)

### 1. Clone the Repository
```bash
git clone https://github.com/youssefkhemiri/House_price_prediction.git
cd House_price_prediction
```

### 2. Set Up Python Environment
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Environment Configuration
```bash
# Create .env file for sensitive data
cp .env.example .env

# Add your OpenAI API key to .env
echo "OPENAI_API_KEY=your_openai_api_key_here" >> .env
```

### 4. Initialize Data Directories
```bash
# Create necessary directories
mkdir -p data/raw data/processed data/external
mkdir -p models logs

# Initialize with empty files to maintain structure
touch data/raw/.gitkeep data/processed/.gitkeep data/external/.gitkeep
```

### 5. Run Initial Tests
```bash
# Run the test suite
python run_tests.py

# Or use pytest directly
pytest tests/ -v
```

### 6. Start the Web Application
```bash
# Start the FastAPI development server
cd src
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

The application will be available at `http://localhost:8000`

### 7. Set Up Automated Crawling (Optional)
```bash
# Install Celery dependencies
pip install -r requirements-celery.txt

# Start Redis server (required for Celery)
# Windows: Download and install Redis from https://redis.io/download
# Linux: sudo systemctl start redis
# Mac: brew services start redis

# Start Celery worker (Terminal 1)
celery -A scripts.crawlers.celery_timer worker --loglevel=info

# Start Celery beat scheduler (Terminal 2)
celery -A scripts.crawlers.celery_timer beat --loglevel=info
```

## üíª Development

### Development Workflow

1. **Feature Development**
   ```bash
   # Create feature branch
   git checkout -b feature/your-feature-name
   
   # Make changes and test
   pytest tests/ -v
   
   # Run the application locally
   uvicorn src.app:app --reload
   ```

2. **Data Collection & Processing**
   ```bash
   # Run scrapers manually
   python scripts/crawlers/menzili_crawler.py
   python scripts/crawlers/mubaweb_crawler.py
   
   # Enrich data with OpenAI
   python scripts/enrich_data.py
   ```

3. **Model Training & Testing**
   ```bash
   # Train new models
   python scripts/train.py
   
   # Test saved models
   python scripts/test_models.py
   
   # Interactive model demo
   python scripts/demo_models.py
   ```

### Code Quality Standards

- **PEP 8**: Follow Python style guidelines
- **Type hints**: Use type annotations for better code clarity
- **Docstrings**: Document all functions and classes
- **Testing**: Maintain 95%+ test coverage
- **Error handling**: Implement comprehensive error handling

### Testing Strategy

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_api.py          # API tests
pytest tests/test_scrapers.py     # Scraper tests
pytest tests/test_prediction.py   # Model tests
pytest tests/test_integration.py  # Integration tests

# Run with coverage report
pytest --cov=src --cov-report=html
```

### Adding New Features

#### Adding a New Scraper
1. Create scraper in `src/scrapers/new_scraper.py`
2. Implement required methods: `scrape_listings()`, `parse_property()`
3. Add tests in `tests/test_scrapers.py`
4. Update web interface in `src/templates/index.html`

#### Adding New ML Models
1. Add model in `scripts/train.py`
2. Update model testing in `scripts/test_models.py`
3. Ensure model serialization compatibility
4. Add performance benchmarks

#### Extending API Endpoints
1. Add endpoint in `src/app.py`
2. Create corresponding tests in `tests/test_api.py`
3. Update API documentation
4. Test with different input scenarios

### Performance Optimization

- **Database**: Consider PostgreSQL for large-scale deployments
- **Caching**: Implement Redis caching for frequent predictions
- **Async processing**: Use FastAPI's async capabilities
- **Model optimization**: Regularly retrain and optimize models
- **Monitoring**: Set up application performance monitoring

### Debugging Tips

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# Run with profiler
python -m cProfile -o profile.stats scripts/train.py

# Monitor Celery tasks
celery -A scripts.crawlers.celery_timer flower --port=5555
```

### Contributing Guidelines

1. **Fork the repository** and create a feature branch
2. **Write tests** for new functionality
3. **Follow code style** guidelines and run linters
4. **Update documentation** for any API changes
5. **Submit a pull request** with detailed description

### Deployment Considerations

- **Environment variables**: Use proper environment management
- **Secrets management**: Secure API keys and credentials
- **Monitoring**: Implement logging and error tracking
- **Scaling**: Consider Docker containerization for production
- **Backup strategy**: Regular data and model backups

---

## üìû Support & Contact

For questions, issues, or contributions:
- **GitHub Issues**: [Create an issue](https://github.com/youssefkhemiri/House_price_prediction/issues)
- **Documentation**: See individual component guides in the docs/ folder
- **Email**: youssefkhemiri@example.com

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

*Built with ‚ù§Ô∏è using Python, FastAPI, and modern ML tools*
